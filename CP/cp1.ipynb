{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #1 (Compilación)\n",
    "\n",
    "A lo largo del curso estaremos implementando un compilador para el lenguaje de programación `HULK` (*Havana University Language for Kompilers*), paso a paso, introduciendo nuevas características del lenguaje o mejorando la implementación de otras características a medida que vamos descubriendo las técnicas fundamentales de la teoría de lenguajes y la compilación.\n",
    "\n",
    "El objetivo de esta clase es construir un evaluador de expresiones \"a mano\", usando los recursos que tenemos hasta el momento. Para ello vamos a comenzar con una versión de `HULK` muy sencilla, un lenguaje de expresiones aritméticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de expresiones\n",
    "\n",
    "Definiremos a continuación este lenguaje de manera informal:\n",
    "\n",
    "Un programa en `HULK` consta de una secuencia de expresiones. Cada expresión está compuesta por:\n",
    "\n",
    "- números (con coma flotante de 32 bits), \n",
    "- operadores `+ *` con el orden operacional, y\n",
    "- paréntesis `(` y `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis lexicográfico\n",
    "\n",
    "Comenzaremos construyendo un prototipo bien simple, donde asumiremos que en la expresión hay espacios en blanco entre todos los elementos, de modo que el *lexer* se reduce a dividir por espacios. Luego iremos adicionando elementos más complejos.\n",
    "\n",
    "El siguiente método devuelve una lista de *tokens*, asumiendo que la expresión solo tiene números, operadores y paréntesis, separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for item in text.split():\n",
    "        # Insert your code here ...\n",
    "        try:\n",
    "            tokens.append(float(item))\n",
    "        except:\n",
    "                tokens.append(item)\n",
    "        pass\n",
    "    return tokens\n",
    "\n",
    "assert tokenize('5 + 6 * 9') == [5, '+', 6, '*', 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis sintáctico y evaluación\n",
    "\n",
    "Una vez que tenemos los *tokens*, solo nos queda evaluar la expresión. Usaremos para ello una idea simple, pero bien útil: evaluaremos recursivamente la expresión descendiendo por los distintos niveles de precedencia.\n",
    "\n",
    "Toda expresión del lenguaje puede ser vista como una suma de _términos_, donde cada uno de estos \"_términos_\" se descompone a su vez en operaciones de multiplicación entre _factores_. Incluso si no hay operadores `+` en toda la expresión queda claro que esta idea es válida puesto que estaríamos en presencia de una expresión formada por un solo _término_. Los _factores_ del lenguaje son todos unidades atómicas: por ahora solo números y expresiones complejas envueltas entre paréntesis. Nótese que el uso de paréntesis permite reiniciar el descenso por los niveles de precedencia (regresar a los niveles más altos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '*': lambda x,y: x * y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util classes and methods\n",
    "\n",
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BadEOFError(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected EOF error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        ParsingError.__init__(self, \"Unexpected EOF\")\n",
    "        \n",
    "class UnexpectedToken(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Unexpected token: {token} at {i}')\n",
    "        \n",
    "class MissingCloseParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing ')' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \")\" token at {i}. Got \"{token}\" instead')\n",
    "        \n",
    "class MissingOpenParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing '(' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \"(\" token at {i}. Got \"{token}\" instead')\n",
    "\n",
    "def get_token(tokens, i, error_type=BadEOFError):\n",
    "    \"\"\"\n",
    "    Returns tokens[i] if 'i' is in range. Otherwise, raises ParsingError exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return tokens[i]\n",
    "    except IndexError:\n",
    "        raise error_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    Evaluates an expression recursively.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i, value = parse_expression(tokens, 0)\n",
    "        assert i == len(tokens)\n",
    "        return value\n",
    "    except ParsingError as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "\n",
    "    if i < len(tokens):\n",
    "        #if tokens[i] == '+':\n",
    "            # Insert your code here ...  \n",
    "        while i < len(tokens):\n",
    "            if(tokens[i] == ')'):\n",
    "                break\n",
    "            operation = operations[tokens[i]]\n",
    "            i, next_factor = parse_term(tokens, i + 1)\n",
    "            term = operation(term, next_factor)\n",
    "    \n",
    "    return i, term\n",
    "        \n",
    "def parse_term(tokens, i):\n",
    "    i, factor = parse_factor(tokens, i)\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == '+' or tokens[i] == ')':\n",
    "            break\n",
    "        operation = operations[tokens[i]]\n",
    "        i, next_factor = parse_factor(tokens, i + 1)\n",
    "        factor = operation(factor, next_factor)\n",
    "    \n",
    "    return i, factor\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "    token = get_token(tokens, i)\n",
    "    \n",
    "    if token == '(':\n",
    "        i, value = parse_expression(tokens, i + 1)\n",
    "        close_parenthesis = get_token(tokens, i)\n",
    "        \n",
    "        if close_parenthesis != ')':\n",
    "            raise MissingCloseParenthesisError(close_parenthesis, i)\n",
    "        \n",
    "        return i + 1, value\n",
    "    else:\n",
    "        if isinstance(token, float):\n",
    "            return i + 1, token\n",
    "        else:\n",
    "            raise UnexpectedToken(token, i)\n",
    "\n",
    "assert evaluate(tokenize('5 + 6 * 9')) == 59.\n",
    "assert evaluate(tokenize('( 5 + 6 ) * 9')) == 99.\n",
    "assert evaluate(tokenize('( 5 + 6 ) + 1 * 9 + 2')) == 22.\n",
    "assert evaluate(tokenize('( ( 5 + 6 ) * 9 ) + 4')) == 103.\n",
    "print(evaluate(tokenize('4 * ( ( 5 + 6 ) + 9 )')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando constantes\n",
    "\n",
    "Agreguemos constantes numéricas al lenguaje `HULK` Para ello, simplemente añadiremos un diccionario con todas las constantes disponibles, que usaremos durante la tokenización. Nótese que solo es necesario modificar el _lexer_ para añadir este rasgo al lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'pi': 3.14159265359,\n",
    "    'e': 2.71828182846,\n",
    "    'phi': 1.61803398875,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    Replaces constants.\n",
    "    \"\"\"\n",
    "    #tokens = []\n",
    "    #\n",
    "    #for token in expr.split():\n",
    "    #    # Insert your code here ...\n",
    "    #    # (You may copy and modify your previous implementation of 'tokenize')\n",
    "    #    pass\n",
    "    tokens = []\n",
    "    for item in expr.split():\n",
    "        # Insert your code here ...\n",
    "        try:\n",
    "            tokens.append(float(item))\n",
    "        except:\n",
    "                if item in constants:\n",
    "                    tokens.append(constants[item])\n",
    "                else:\n",
    "                    tokens.append(item)\n",
    "        \n",
    "    return tokens\n",
    "    \n",
    "assert tokenize('2 * pi') == [2.0, '*', 3.14159265359]\n",
    "assert evaluate(tokenize('2 * pi')) == 6.28318530718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando operadores `-` y `/`\n",
    "\n",
    "- **Restricción:** No utilizar ciclos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '-': lambda x,y: x - y,\n",
    "    '*': lambda x,y: x * y,\n",
    "    '/': lambda x,y: x / y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def parse_expression(tokens, i):\n",
    "#    # Insert your code here ...\n",
    "#    i, term = parse_term(tokens, i)\n",
    "#\n",
    "#    if i < len(tokens):\n",
    "#        #if tokens[i] == '+':\n",
    "#            # Insert your code here ...  \n",
    "#        while i < len(tokens):\n",
    "#            if(tokens[i] == ')' or tokens[i] == ','):\n",
    "#                break\n",
    "#            operation = operations[tokens[i]]\n",
    "#            i, next_factor = parse_term(tokens, i + 1)\n",
    "#            term = operation(term, next_factor)\n",
    "#    \n",
    "#    return i, term\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "    \n",
    "    if i < len(tokens) and tokens[i] != ')' and tokens[i] != ',':\n",
    "        i, term = parse_exp1(tokens, i, term)\n",
    "    \n",
    "    return i, term\n",
    "\n",
    "def parse_exp1(tokens, i, term):\n",
    "    if i >= len(tokens) or tokens[i] == ')' or tokens[i] == ',':\n",
    "        return i, term\n",
    "    operation = operations[tokens[i]]\n",
    "    i, next_factor = parse_term(tokens, i + 1)\n",
    "    term = operation(term, next_factor)\n",
    "    return parse_exp1(tokens, i, term)\n",
    "\n",
    "#def parse_term(tokens, i):\n",
    "#    # Insert your code here ...\n",
    "#    i, factor = parse_factor(tokens, i)\n",
    "#    \n",
    "#    while i < len(tokens):\n",
    "#        if tokens[i] == '+' or tokens[i] == ')' or tokens[i] == '-' or tokens[i] == ',':\n",
    "#            break\n",
    "#        operation = operations[tokens[i]]\n",
    "#        i, next_factor = parse_factor(tokens, i + 1)\n",
    "#        factor = operation(factor, next_factor)\n",
    "#    \n",
    "#    return i, factor\n",
    "def parse_term(tokens, i):\n",
    "    i, factor = parse_factor(tokens, i)\n",
    "    if i < len(tokens) and tokens[i] != '+' and tokens[i] != ')' and tokens[i] != '-' and tokens[i] != ',':\n",
    "        i, factor = parse_t1(tokens, i, factor)\n",
    "    return i, factor\n",
    "\n",
    "\n",
    "def parse_t1(tokens, i, factor):\n",
    "    if i >= len(tokens) or tokens[i] == '+' or tokens[i] == ')' or tokens[i] == '-' or tokens[i] == ',':\n",
    "        return i, factor\n",
    "    operation = operations[tokens[i]]\n",
    "    i, next_factor = parse_factor(tokens, i + 1)\n",
    "    factor = operation(factor, next_factor)\n",
    "    return i, factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate(tokenize('1 - 1 + 1')) == 1\n",
    "assert evaluate(tokenize('8 / 4 / 2')) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_.\n",
    "\n",
    "Para las funciones elementales haremos algo similar a las constantes, pero en vez de a la hora de tokenizar, las reemplazaremos a la hora de evaluar, pues necesitamos evaluar recursivamente los argumentos de la función. Empezaremos por garantizar que nuestro tokenizador que es capaz de reconocer expresiones con funciones elementales de más de un argumento, en caso de no ser así es necesario arreglarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize('log ( 64 , 1 + 3 )') == ['log', '(', 64.0, ',', 1.0, '+', 3.0, ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionaremos entonces un diccionario con todas las funciones elementales que permitiremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, modificaremos el método `evaluate` para que use las funciones elementales. Recordemos que los argumentos están separados por el token _coma_ (`,`) y que cada uno puede a su vez tener sub-expresiones que consistan también en llamados a funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765.0\n"
     ]
    }
   ],
   "source": [
    "def parse_factor(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    # (You may copy and modify your previous implementation of 'parse_factor')\n",
    "    token = get_token(tokens, i)\n",
    "    \n",
    "    if token == '(':\n",
    "        i, value = parse_expression(tokens, i + 1)\n",
    "        close_parenthesis = get_token(tokens, i)\n",
    "        \n",
    "        if close_parenthesis != ')':\n",
    "            raise MissingCloseParenthesisError(close_parenthesis, i)\n",
    "        \n",
    "        return i + 1, value\n",
    "\n",
    "    elif token in functions and token != 'log':\n",
    "        function = functions[tokens[i]]\n",
    "        i = i + 1\n",
    "\n",
    "        if tokens[i] != '(':\n",
    "            raise MissingOpenParenthesisError(token[i], i)\n",
    "\n",
    "        i, value = parse_expression(tokens, i + 1)\n",
    "        close_parenthesis = get_token(tokens, i)\n",
    "        \n",
    "        if close_parenthesis != ')':\n",
    "            raise MissingCloseParenthesisError(close_parenthesis, i)\n",
    "\n",
    "        return i + 1, function(value)\n",
    "\n",
    "    elif token == 'log':\n",
    "        function = functions[tokens[i]]\n",
    "        i = i + 1\n",
    "\n",
    "        if tokens[i] != '(':\n",
    "            raise MissingOpenParenthesisError(token[i], i)\n",
    "\n",
    "        i, value1 = parse_expression(tokens, i + 1)\n",
    "        if tokens[i] != ',':\n",
    "            raise UnexpectedToken(token, i)\n",
    "\n",
    "        i, value2 = parse_expression(tokens, i + 1)\n",
    "        close_parenthesis = get_token(tokens, i)\n",
    "        if close_parenthesis != ')':\n",
    "            raise MissingCloseParenthesisError(close_parenthesis, i)\n",
    "\n",
    "        return i + 1, function(value1, value2)\n",
    "    else:\n",
    "        if isinstance(token, float):\n",
    "            return i + 1, token\n",
    "        else:\n",
    "            raise UnexpectedToken(token, i)\n",
    "\n",
    "    \n",
    "assert evaluate(tokenize('log ( 64 , 1 + 3 )')) == 3.0\n",
    "assert evaluate(tokenize('sqrt ( 1 + 3 ) + 4')) == 6.0\n",
    "print(evaluate(tokenize('5 * ( ( 6 + 3 ) * ( 9 + 8 ) )')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
